{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1xVjrMX0x-kp",
        "outputId": "b5938755-5f32-4732-ae63-fe3daeec3e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.12.8-py3-none-any.whl.metadata (180 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/180.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.7.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "INFO: pip is looking at multiple versions of facenet-pytorch to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading facenet_pytorch-2.5.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (0.24.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (11.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->facenet-pytorch) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision->facenet-pytorch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision->facenet-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision->facenet-pytorch) (3.0.3)\n",
            "Downloading yt_dlp-2025.12.8-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp, facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.3 yt-dlp-2025.12.8\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp moviepy imageio-ffmpeg openai opencv-python facenet-pytorch safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5KTUazbHxey_"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from openai import OpenAI\n",
        "import uuid\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# ============================================\n",
        "# 영상 → 프레임 추출 함수\n",
        "# ============================================\n",
        "def extract_frames(video_path: str, output_dir: str, num_frames: int = 10) -> List[str]:\n",
        "    \"\"\"\n",
        "    영상의 앞에서 num_frames만큼 프레임을 추출하는 함수 (OpenCV 사용)\n",
        "\n",
        "    Args:\n",
        "        video_path (str): 입력 영상 경로\n",
        "        output_dir (str): 추출한 프레임 저장 경로\n",
        "        num_frames (int): 뽑을 프레임 수 (기본 10)\n",
        "\n",
        "    Returns:\n",
        "        List[str]: 저장된 프레임 이미지 경로 리스트\n",
        "    \"\"\"\n",
        "    video_path = str(video_path)\n",
        "    output_dir = Path(output_dir)\n",
        "\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Cannot open video: {video_path}\")\n",
        "\n",
        "    extracted_paths = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # total_frames보다 num_frames이 큰 경우 조절\n",
        "    actual_frames = min(num_frames, total_frames)\n",
        "\n",
        "    # 프레임 순차 추출\n",
        "    for i in range(actual_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_path = output_dir / f\"frame_{i:03d}.jpg\"\n",
        "        cv2.imwrite(str(frame_path), frame)\n",
        "        extracted_paths.append(str(frame_path))\n",
        "\n",
        "    cap.release()\n",
        "    return extracted_paths\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 오디오 추출 + Whisper STT\n",
        "# ============================================\n",
        "\n",
        "# OpenAI Whisper API 클라이언트 생성\n",
        "client = OpenAI(api_key=\"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\")\n",
        "\n",
        "def extract_audio(video_path, output_audio_path=None, audio_format=\"mp3\", whisper_model=\"whisper-1\"):\n",
        "    \"\"\"\n",
        "    1) 비디오에서 FFmpeg로 오디오 추출\n",
        "    2) Whisper STT 적용\n",
        "    3) 텍스트 반환\n",
        "\n",
        "    :param video_path: 입력 영상 경로\n",
        "    :param output_audio_path: 출력 오디오 파일명 (없으면 자동 생성)\n",
        "    :param audio_format: mp3 또는 wav\n",
        "    :param whisper_model: Whisper 모델 이름\n",
        "    \"\"\"\n",
        "    video_path = Path(video_path)\n",
        "\n",
        "    # 출력 오디오 파일명 지정\n",
        "    if output_audio_path is None:\n",
        "        output_audio_path = video_path.with_suffix(f\".{audio_format}\")\n",
        "    output_audio_path = Path(output_audio_path)\n",
        "\n",
        "    # ----------------------------\n",
        "    # 1. moviepy 오디오 추출 (imageio-ffmpeg 백엔드 사용)\n",
        "    # ----------------------------\n",
        "    try:\n",
        "        # 1. VideoFileClip 객체 생성\n",
        "        video_clip = VideoFileClip(str(video_path))\n",
        "\n",
        "        # 2. 오디오 클립 추출\n",
        "        audio_clip = video_clip.audio\n",
        "\n",
        "        # 3. 오디오 파일로 저장\n",
        "        # audio_clip.write_audiofile()이 내부적으로 imageio-ffmpeg을 통해 FFmpeg 명령을 실행합니다.\n",
        "        audio_clip.write_audiofile(\n",
        "            str(output_audio_path),\n",
        "            codec=audio_format, # moviepy는 파일 확장자에 따라 코덱을 자동 선택\n",
        "            logger=None           # 터미널에 FFmpeg 로그 출력을 숨겨서 깔끔하게 만듭니다.\n",
        "        )\n",
        "\n",
        "        # 4. 클립 자원 해제\n",
        "        audio_clip.close()\n",
        "        video_clip.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 오디오 추출 실패 (moviepy): {e}\")\n",
        "        return # 추출 실패 시 함수 종료\n",
        "    # ----------------------------\n",
        "    # 2. Whisper API로 STT 수행\n",
        "    # ----------------------------\n",
        "    with open(output_audio_path, \"rb\") as audio_file:\n",
        "        result = client.audio.transcriptions.create(\n",
        "            model=whisper_model,  # Whisper 모델 예: \"whisper-1\"\n",
        "            file=audio_file,\n",
        "            language=\"ko\",\n",
        "        )\n",
        "\n",
        "    text = result.text\n",
        "    return text\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 텍스트 → 문장 단위 분리\n",
        "# ============================================\n",
        "def split_sentences(paragraph: str):\n",
        "    \"\"\"\n",
        "    문단을 '.!? ' 기준으로 문장 단위로 분리하여 리스트로 반환\n",
        "    \"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', paragraph.strip())\n",
        "    return [s.strip() for s in sentences if s.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg6FC7HMyIz0",
        "outputId": "3eb8bf48-c689-405d-ae27-a50aaa0b343c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] 영상 모델 로드 완료\n",
            "[INFO] 텍스트 모델 로드 완료\n"
          ]
        }
      ],
      "source": [
        "# models.py\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from facenet_pytorch import MTCNN\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "# ============================================\n",
        "# MobileNetV3 + LSTM 구조 (영상 딥페이크 탐지 모델)\n",
        "# ============================================\n",
        "\n",
        "IMG_SIZE = 256\n",
        "BASE_DIR = Path(\"/content\")\n",
        "MODEL_PATH = BASE_DIR / \"weights\" / \"mobilenetv3_lstm_best_freeze_optuna_acc.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class MobileNetV3LSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 1,\n",
        "        latent_dim: int = 960,\n",
        "        lstm_layers: int = 1,\n",
        "        hidden_dim: int = 2048,\n",
        "        bidirectional: bool = False,\n",
        "        dropout: float = 0.4\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # MobileNetV3 Large 사전학습 모델 로드\n",
        "        # features 부분만 사용 (고정된 feature extractor)\n",
        "        base = models.mobilenet_v3_large(pretrained=True)\n",
        "        self.feature_extractor = base.features\n",
        "\n",
        "        # -----------------------------------\n",
        "        # CNN backbone freeze (추론 시에도 그대로 사용)\n",
        "        # -----------------------------------\n",
        "        for param in self.feature_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "        # 마지막 block만 unfreeze 할 수 있으나, inference에는 영향 없음\n",
        "\n",
        "        # 영상 프레임 feature → 1×1 평균 풀링\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # 시간적 연속성을 위한 LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        lstm_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "\n",
        "        # Fully-connected classifier\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(lstm_out_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: 영상 시퀀스 (B, T, C, H, W)\n",
        "        \"\"\"\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        # (B*T, C, H, W) 형태로 CNN 통과\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        fmap = self.feature_extractor(x)\n",
        "\n",
        "        # (B*T, latent_dim)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(B, T, -1)\n",
        "\n",
        "        # LSTM으로 temporal feature 처리\n",
        "        x_lstm, _ = self.lstm(x)\n",
        "\n",
        "        # 모든 time-step 평균\n",
        "        x = torch.mean(x_lstm, dim=1)\n",
        "\n",
        "        # 최종 binary logit 출력\n",
        "        x = self.fc(self.dropout(self.relu(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 영상 모델 로드 함수\n",
        "# ============================================\n",
        "def load_trained_model(model_path: Path, device: torch.device):\n",
        "    \"\"\"저장된 가중치 파일(.pth)을 로드하여 모델 복원\"\"\"\n",
        "    assert model_path.is_file(), f\"모델 파일이 없습니다: {model_path}\"\n",
        "\n",
        "    model = MobileNetV3LSTM()\n",
        "    ckpt = torch.load(model_path, map_location=device)\n",
        "\n",
        "    # 다양한 저장 형태 지원\n",
        "    if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n",
        "        state_dict = ckpt[\"model_state_dict\"]\n",
        "    else:\n",
        "        state_dict = ckpt\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"[INFO] 영상 모델 로드 완료\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 텍스트 과장광고 분류 모델 (KcELECTRA)\n",
        "# ============================================\n",
        "\n",
        "BASE_MODEL_NAME = \"beomi/KcELECTRA-base-v2022\"\n",
        "CHECKPOINT_ROOT = \"/content/drive/MyDrive/Colab Notebooks/텍스트/BCE/checkpoints/[최종]KcELECTRA(BCE_마지막 레이어 4층만 unfreeze_X축 step_파리미터 기본값)/checkpoint-150\"\n",
        "\n",
        "class TextExaggerationClassifier:\n",
        "    \"\"\"\n",
        "    KcELECTRA 기반 단문 과장광고 분류기\n",
        "    - 입력: 문장(str)\n",
        "    - 출력: 정상(1)일 확률\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 checkpoint_root: str = CHECKPOINT_ROOT,\n",
        "                 base_model_name: str = BASE_MODEL_NAME,\n",
        "                 device: torch.device = device):\n",
        "\n",
        "        # 토크나이저 로드\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=False)\n",
        "\n",
        "        # 사전 fine-tuned 된 모델 로드\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT_ROOT)\n",
        "\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "        self.device = device\n",
        "\n",
        "        print(f\"[INFO] 텍스트 모델 로드 완료\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_proba(self, texts):\n",
        "        \"\"\"\n",
        "        입력 문장 리스트에 대해 정상 확률 반환\n",
        "        \"\"\"\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        enc = {k: v.to(self.device) for k, v in enc.items()}\n",
        "\n",
        "        outputs = self.model(**enc)\n",
        "\n",
        "        # binary classification → sigmoid 적용\n",
        "        logits = outputs.logits.squeeze(-1)\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        return probs.cpu().numpy().tolist()\n",
        "\n",
        "    def predict_label(self, texts, threshold: float = 0.5):\n",
        "        \"\"\"\n",
        "        threshold 기준으로 정상/과장 라벨(1/0) 반환\n",
        "        \"\"\"\n",
        "        probs = self.predict_proba(texts)\n",
        "        labels = [1 if p >= threshold else 0 for p in probs]\n",
        "        return labels\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# Models 클래스: 영상 + 텍스트 모델 통합\n",
        "# ============================================\n",
        "class Models:\n",
        "    def __init__(self):\n",
        "        # 1) 영상 딥페이크 모델 로드\n",
        "        self.model = load_trained_model(MODEL_PATH, device)\n",
        "\n",
        "        # 2) 얼굴 검출용 MTCNN\n",
        "        self.mtcnn = MTCNN(\n",
        "            image_size=IMG_SIZE,\n",
        "            margin=20,\n",
        "            keep_all=False,\n",
        "            device=device,\n",
        "            post_process=False\n",
        "        )\n",
        "\n",
        "        # 3) 기본 이미지 전처리\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.normalize = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "\n",
        "        # 4) 디바이스\n",
        "        self.device = device\n",
        "\n",
        "        # 5) 텍스트 과장광고 분류 모델\n",
        "        self.text_classifier = TextExaggerationClassifier()\n",
        "\n",
        "\n",
        "    # ============================================\n",
        "    # 영상 예측: 가짜 확률(p_fake) 반환\n",
        "    # ============================================\n",
        "    def predict_video(self, frame_dir: str, seq_len: int = 10):\n",
        "        frame_dir = Path(frame_dir)\n",
        "\n",
        "        # jpg/png 프레임 로드\n",
        "        frame_paths = sorted(frame_dir.glob(\"*.jpg\")) + \\\n",
        "                      sorted(frame_dir.glob(\"*.png\"))\n",
        "\n",
        "        if len(frame_paths) == 0:\n",
        "            raise RuntimeError(f\"No frames found in {frame_dir}\")\n",
        "\n",
        "        faces = []\n",
        "\n",
        "        # 최대 seq_len 프레임만 사용\n",
        "        for i, frame_path in enumerate(frame_paths[:seq_len]):\n",
        "            img = Image.open(frame_path).convert(\"RGB\")\n",
        "\n",
        "            # 얼굴 검출 (없으면 resize fallback)\n",
        "            with torch.no_grad():\n",
        "                face = self.mtcnn(img)\n",
        "\n",
        "            if face is None:\n",
        "                # 얼굴이 없으면 전체 프레임을 resize하여 사용\n",
        "                img_resized = img.resize((256, 256))\n",
        "                face = self.to_tensor(img_resized)\n",
        "\n",
        "                # 디버깅용 저장\n",
        "                output_path = frame_dir / f\"resized_{i:03d}.jpg\"\n",
        "                img_resized.save(output_path)\n",
        "            else:\n",
        "                # mtcnn 출력이 다양한 형태일 수 있음 → Tensor로 정규화\n",
        "                if isinstance(face, np.ndarray):\n",
        "                    face = torch.from_numpy(face)\n",
        "                if isinstance(face, Image.Image):\n",
        "                    face = self.to_tensor(face)\n",
        "                if face.max() > 1:\n",
        "                    face = face / 255.0\n",
        "                if face.ndim == 4:\n",
        "                    face = face[0]\n",
        "\n",
        "                # crop된 얼굴 저장(디버깅)\n",
        "                if isinstance(face, torch.Tensor):\n",
        "                    face_img = transforms.ToPILImage()(face.cpu().clamp(0.0, 1.0))\n",
        "                    output_path = frame_dir / f\"face_cropped_{i:03d}.jpg\"\n",
        "                    face_img.save(output_path)\n",
        "\n",
        "            # Normalize 적용\n",
        "            face = self.normalize(face)\n",
        "            faces.append(face)\n",
        "\n",
        "        # 프레임이 부족하면 마지막 프레임 반복\n",
        "        while len(faces) < seq_len:\n",
        "            faces.append(faces[-1].clone())\n",
        "\n",
        "        faces_tensor = torch.stack(faces, dim=0)          # (T, C, H, W)\n",
        "        faces_tensor = faces_tensor.unsqueeze(0).to(self.device)  # (1, T, C, H, W)\n",
        "\n",
        "        # 추론\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(faces_tensor)\n",
        "            p_real = torch.sigmoid(logits).item()\n",
        "            p_fake = 1 - p_real\n",
        "\n",
        "        return round(p_fake, 4)\n",
        "\n",
        "\n",
        "    # ============================================\n",
        "    # 텍스트 예측: 문장별 과장 확률 등을 포함한 결과 반환\n",
        "    # ============================================\n",
        "    def predict_text(self, text: str):\n",
        "        sentences = split_sentences(text)\n",
        "\n",
        "        if len(sentences) == 0:\n",
        "            return {\n",
        "                \"most_exaggerated_sentence\": None,\n",
        "                \"exaggeration_prob\": 0.0,\n",
        "                \"sentence_probs\": []\n",
        "            }\n",
        "\n",
        "        # 문장별 정상 확률 계산\n",
        "        p_normals = self.text_classifier.predict_proba(sentences)\n",
        "\n",
        "        results = []\n",
        "        for sent, p_normal in zip(sentences, p_normals):\n",
        "            p_exag = 1 - p_normal\n",
        "            results.append((sent, float(p_normal), float(p_exag)))\n",
        "\n",
        "        # 과장 확률이 가장 높은 문장 선택\n",
        "        most_exaggerated = max(results, key=lambda x: x[2])\n",
        "\n",
        "        return {\n",
        "            \"most_exaggerated_sentence\": most_exaggerated[0],\n",
        "            \"exaggeration_prob\": most_exaggerated[2],\n",
        "            \"sentence_probs\": results\n",
        "        }\n",
        "\n",
        "# 전역 인스턴스 (tasks 및 main에서 import하여 사용)\n",
        "models = Models()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slwAA0LnzXK3"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import json\n",
        "import os\n",
        "\n",
        "# ==============================================\n",
        "# 1) 로컬 영상 분석 - 프레임 추출 + 딥페이크 점수 계산\n",
        "# ==============================================\n",
        "def task_video_local(video_path, unique_id, num_frames=10):\n",
        "    # 고유 ID 기반 temp 폴더 생성\n",
        "    base_dir = f\"./temp/{unique_id}\"\n",
        "    frame_dir = f\"{base_dir}/frames\"\n",
        "    os.makedirs(frame_dir, exist_ok=True)\n",
        "\n",
        "    # 영상 → 프레임 추출\n",
        "    extract_frames(video_path, frame_dir, num_frames)\n",
        "\n",
        "    # 모델로 프레임 분석 → 딥페이크 점수 반환\n",
        "    score = models.predict_video(frame_dir)\n",
        "    return score\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# 2) 로컬 오디오 분석 - 음성 추출 + 텍스트 분석\n",
        "# ==============================================\n",
        "def task_audio_local(video_path, unique_id):\n",
        "    # 고유 ID 기반 temp 폴더 생성\n",
        "    base_dir = f\"./temp/{unique_id}\"\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    # 영상 → 오디오 파일(mp3) 추출\n",
        "    audio_path = f\"{base_dir}/audio.mp3\"\n",
        "    transcript = extract_audio(video_path, audio_path)\n",
        "\n",
        "    # 텍스트 기반 과장광고 확률 계산\n",
        "    result = models.predict_text(transcript)\n",
        "\n",
        "    # exaggeration_prob 값을 float 형태로 반환\n",
        "    score_value = result.get(\"exaggeration_prob\")\n",
        "    return float(score_value)\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# 3) 딥페이크 + 과장광고 점수 → 위험 레이블 결정 + Late Fusion\n",
        "# ==============================================\n",
        "def combine_results_local(video_score, text_score):\n",
        "    # 위험도 등급 분류\n",
        "    if text_score >= 0.5 and video_score >= 0.5:\n",
        "        danger_class = \"매우위험\"\n",
        "    elif text_score >= 0.5:\n",
        "        danger_class = \"위험\"\n",
        "    elif video_score >= 0.5:\n",
        "        danger_class = \"주의\"\n",
        "    else:\n",
        "        danger_class = \"안전\"\n",
        "\n",
        "    # Late Fusion (가중 평균 후 inverse: 안전할수록 높게 나오지 않도록 뒤집음)\n",
        "    joint_score = 0.7 * video_score + 0.3 * text_score\n",
        "    final_score = 1 - joint_score\n",
        "\n",
        "    return {\n",
        "        \"video_score\": video_score,\n",
        "        \"text_score\": text_score,\n",
        "        \"label\": danger_class,\n",
        "        \"final_score\": final_score\n",
        "    }\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# 4) 전체 로컬 인퍼런스 파이프라인 실행\n",
        "#    (프레임 분석 + 오디오 분석 → Late Fusion)\n",
        "# ==============================================\n",
        "def run_inference_local(video_path):\n",
        "    # temp 디렉토리 분리를 위한 UUID 생성\n",
        "    unique_id = str(uuid.uuid4())\n",
        "\n",
        "    # ① 영상 기반 딥페이크 점수\n",
        "    video_score = task_video_local(video_path, unique_id)\n",
        "\n",
        "    # ② 오디오 기반 과장광고 점수\n",
        "    text_score = task_audio_local(video_path, unique_id)\n",
        "\n",
        "    # ③ 두 결과 결합 + 레이블 결정\n",
        "    result = combine_results_local(video_score, text_score)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQv85o3tzyTJ",
        "outputId": "12e1b572-695e-47d9-bc47-35806ee7a6a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'video_score': 0.8539,\n",
              " 'text_score': 0.9631876200437546,\n",
              " 'label': '매우위험',\n",
              " 'final_score': 0.11331371398687362}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# sample data 폴더 영상 사용\n",
        "video_path = \"/content/test.mp4\"\n",
        "\n",
        "# inference 실행\n",
        "result = run_inference_local(video_path)\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
